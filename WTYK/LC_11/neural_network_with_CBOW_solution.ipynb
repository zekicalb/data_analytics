{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Continuous Bag of Words (CBOW)\n",
    "\n",
    "For details see: https://www.kaggle.com/code/alincijov/nlp-starter-continuous-bag-of-words-cbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence\n",
    "\n",
    "Source: https://en.wikipedia.org/wiki/Currywurst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file in read mode ('r')\n",
    "with open('text.txt', 'r') as file:\n",
    "\n",
    "    # Read the contents of the file\n",
    "    sentences = file.read()\n",
    "\n",
    "# Print the file content\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a word cloud\n",
    "def wordCloud_generator(data, title=None):\n",
    "    \n",
    "    wordcloud = WordCloud(height=300,\n",
    "                          width=600,\n",
    "                          background_color ='white',\n",
    "                          min_font_size = 8\n",
    "                         ).generate(\" \".join(data))\n",
    "    \n",
    "    # Plot the WordCloud image                        \n",
    "    plt.figure(figsize = (6, 4), facecolor = None) \n",
    "    plt.imshow(wordcloud, interpolation='bilinear') \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.title(title,fontsize=16)\n",
    "    plt.show()\n",
    "    \n",
    "# Create Wordcloud\n",
    "wordCloud_generator(sentences, title=\"Word cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)\n",
    "\n",
    "# Remove 1 letter words\n",
    "sentences = re.sub(r'(?:^| )\\w(?:$| )', ' ', sentences).strip()\n",
    "\n",
    "# Lower all characters\n",
    "sentences = sentences.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentense to derive vocabulary (tokens)\n",
    "words = sentences.split()\n",
    "vocab = set(words)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "context_size = 2\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data bags with ([context], target) as the basis for modeling\n",
    "data = []\n",
    "\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1], words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "# Show first entries in 'data'\n",
    "data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word embeddings\n",
    "np.random.seed(42)\n",
    "embeddings = np.random.random_sample((vocab_size, embed_dim))\n",
    "\n",
    "# Show first embeddings\n",
    "print(embeddings[0:5])\n",
    "\n",
    "# Dimensions of the array 'np.ndarray'\n",
    "print('\\nDimensions of np.ndarray:', embeddings.ndim)\n",
    "\n",
    "# Shape of the array 'np.ndarray'\n",
    "print('\\nShape of np.ndarray:', embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform linear transformation of word embeddings\n",
    "def linear(m, theta):\n",
    "    w = theta\n",
    "\n",
    "    return m.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log softmax + NLLLoss = Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function to measure the model performance during training\n",
    "def log_softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "\n",
    "    return np.log(e_x / e_x.sum())\n",
    "\n",
    "def NLLLoss(logs, targets):\n",
    "    out = logs[range(len(targets)), targets]\n",
    "\n",
    "    return -out.sum()/len(out)\n",
    "\n",
    "def log_softmax_crossentropy_with_logits(logits,target):\n",
    "    out = np.zeros_like(logits)\n",
    "    out[np.arange(len(logits)),target] = 1\n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- out + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to provide a forward pass through a neural network\n",
    "def forward(context_idxs, theta):\n",
    "    m = embeddings[context_idxs].reshape(1, -1)\n",
    "    n = linear(m, theta)\n",
    "    o = log_softmax(n)\n",
    "    \n",
    "    return m, n, o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to provide a backward pass through a neural network\n",
    "def backward(preds, theta, target_idxs):\n",
    "    m, n, o = preds\n",
    "    \n",
    "    dlog = log_softmax_crossentropy_with_logits(n, target_idxs)\n",
    "    dw = m.T.dot(dlog)\n",
    "    \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for parameter (theta) optimization\n",
    "def optimize(theta, grad, lr=0.03):\n",
    "    theta -= grad * lr\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the neural network\n",
    "theta = np.random.uniform(-1, 1, (2 * context_size * embed_dim, vocab_size))\n",
    "\n",
    "epoch_losses = {}\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    losses =  []\n",
    "\n",
    "    for context, target in data:\n",
    "        context_idxs = np.array([word_to_ix[w] for w in context])\n",
    "        preds = forward(context_idxs, theta)\n",
    "\n",
    "        target_idxs = np.array([word_to_ix[target]])\n",
    "        loss = NLLLoss(preds[-1], target_idxs)\n",
    "\n",
    "        losses.append(loss)\n",
    "\n",
    "        grad = backward(preds, theta, target_idxs)\n",
    "        theta = optimize(theta, grad, lr=0.03)\n",
    "    \n",
    "    epoch_losses[epoch] = losses\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot loss/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the neural network\n",
    "ix = np.arange(0,100)\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "fig.suptitle('Epoch/Losses', fontsize=14)\n",
    "plt.plot(ix,[epoch_losses[i][0] for i in ix], c='orange')\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Losses', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the target word\n",
    "def predict(words):\n",
    "    \n",
    "    context_idxs = np.array([word_to_ix[w] for w in words])\n",
    "    preds = forward(context_idxs, theta)\n",
    "    word = ix_to_word[np.argmax(preds[-1])]\n",
    "    \n",
    "    return word\n",
    "\n",
    "# Predict target word based on input words\n",
    "print(predict(['according', 'to', 'belief', 'the']))\n",
    "print(predict(['the', 'post', 'war', 'ii']))\n",
    "print(predict(['originated', 'in', 'in', 'the']))\n",
    "print(predict(['the', 'currywurst', 'in', 'berlin']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict all targets of all data bags and show accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "def accuracy():\n",
    "    wrong = 0\n",
    "    for context, target in data:\n",
    "        if(predict(context) != target):\n",
    "            wrong += 1\n",
    "            \n",
    "    return (1 - (wrong / len(data)))\n",
    "\n",
    "print(f'{accuracy():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
